{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "import cPickle as pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# map labels from {-1, 1} to {0, 1}\n",
    "labels, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,eval_metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65184641,  0.34815356],\n",
       "       [ 0.74603701,  0.25396299],\n",
       "       [ 0.32435191,  0.67564809],\n",
       "       ..., \n",
       "       [ 0.71962714,  0.28037286],\n",
       "       [ 0.08567679,  0.91432321],\n",
       "       [ 0.01836032,  0.98163968]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Load data into a data frame for use in running model\n",
    "    '''\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    '''Stem the tokens.'''\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def OHStokenize(text):\n",
    "    '''Tokenize & stem. Stems automatically for now.\n",
    "    Leaving \"stemmer\" out of function call, so it works with TfidfVectorizer'''\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "###########################################################################\n",
    "# tokenization code\n",
    "\n",
    "def seperatePunct(incomingString):\n",
    "    outstr = ''\n",
    "    characters = set(['!','@','#','$',\"%\",\"^\",\"&\",\"*\",\":\",\"\\\\\",\n",
    "                  \"(\",\")\",\"+\",\"=\",\"?\",\"\\'\",\"\\\"\",\";\",\"/\",\n",
    "                  \"{\",\"}\",\"[\",\"]\",\"<\",\">\",\"~\",\"`\",\"|\"])\n",
    "\n",
    "    for char in incomingString:\n",
    "        if char in characters:\n",
    "            outstr = outstr + ' ' + char + ' '\n",
    "        else:\n",
    "            outstr = outstr + char\n",
    "\n",
    "    return outstr\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "     return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def text_cleaner(wordList):\n",
    "    '''\n",
    "    INPUT: List of words to be tokenized\n",
    "    OUTPUT: List of tokenized words\n",
    "    '''\n",
    "\n",
    "    tokenziedList = []\n",
    "\n",
    "    for word in wordList:\n",
    "\n",
    "        #remove these substrings from the word\n",
    "        word = word.replace('[deleted]','')\n",
    "        word = word.replace('&gt','')\n",
    "\n",
    "        #if link, replace with linktag\n",
    "        if 'http' in word:\n",
    "            tokenziedList.append('LINK_TAG')\n",
    "            continue\n",
    "\n",
    "        #if reference to subreddit, replace with reddittag\n",
    "        if '/r/' in word:\n",
    "            tokenziedList.append('SUBREDDIT_TAG')\n",
    "            continue\n",
    "\n",
    "        #if reference to reddit user, replace with usertag\n",
    "        if '/u/' in word:\n",
    "            tokenziedList.append('USER_TAG')\n",
    "            continue\n",
    "\n",
    "        #if reference to twitter user, replace with usertag\n",
    "        if '@' in word:\n",
    "            tokenziedList.append('USER_TAG')\n",
    "            continue\n",
    "\n",
    "        #if number, replace with numtag\n",
    "        #m8 is a word, 5'10\" and 54-59, 56:48 are numbers\n",
    "        if hasNumbers(word) and not any(char.isalpha() for char in word):\n",
    "            tokenziedList.append('NUM_TAG')\n",
    "            continue\n",
    "\n",
    "        #seperate puncuations and add to tokenizedList\n",
    "        newwords = seperatePunct(word).split(\" \")\n",
    "        tokenziedList.extend(newwords)\n",
    "\n",
    "    return tokenziedList\n",
    "\n",
    "def mytokenize(comment):\n",
    "    '''\n",
    "    Input: takes in a reddit comment as a str or unicode and tokenizes it\n",
    "    Output: a tokenized list\n",
    "    '''\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    sentenceList = tokenizer.tokenize(comment)\n",
    "    wordList = []\n",
    "    for sentence in sentenceList:\n",
    "        wordList.extend(sentence.split(\" \"))\n",
    "\n",
    "    return text_cleaner(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../../data/labeledRedditComments2.p'\n",
    "cvpath = '../../data/twitter_cross_val.csv'\n",
    "\n",
    "df = pickle.load(open(path, 'rb'))\n",
    "dfcv = pd.read_csv(cvpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take a subset of the data for testing this code\n",
    "randNums = np.random.randint(low=0,high=len(df.index),size=(200,1))\n",
    "rowList = [int(row) for row in randNums]\n",
    "dfsmall = df.ix[rowList,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nf = dfsmall\n",
    "X = nf.body\n",
    "y = nf.label\n",
    "\n",
    "Xcv = dfcv['tweet_text'].values\n",
    "ycv = dfcv['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english', decode_error='ignore',\n",
    "                           tokenizer=mytokenize)\n",
    "\n",
    "\n",
    "# fit & transform comments matrix\n",
    "tfidf_X = vect.fit_transform(X)\n",
    "tfidf_Xcv = vect.transform(Xcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2384)\n",
      "(10000, 2384)\n"
     ]
    }
   ],
   "source": [
    "print tfidf_X.shape\n",
    "print tfidf_Xcv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_Xcvd = tfidf_Xcv.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(tfidf_X, label=y)\n",
    "xg_cv = xgb.DMatrix(tfidf_Xcv, label=ycv)\n",
    "xg_cvd = xgb.DMatrix(tfidf_Xcvd, label=ycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2384\n",
      "2381\n",
      "2384\n"
     ]
    }
   ],
   "source": [
    "print xg_train.num_col()\n",
    "print xg_cv.num_col()\n",
    "print xg_cvd.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
